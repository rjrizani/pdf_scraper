{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rjrizani/pdf_scraper/blob/main/PDF_Metadata_Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ5xKgh4-ASM",
        "outputId": "c4b0ba74-5a1f-41a8-d6e0-686f175544f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting requests\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests)\n",
            "  Using cached charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl.metadata (36 kB)\n",
            "Collecting idna<4,>=2.5 (from requests)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests)\n",
            "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "Using cached charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl (102 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
            "Successfully installed certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 requests-2.32.3 urllib3-2.3.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting beautifulsoup4\n",
            "  Using cached beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
            "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typing-extensions>=4.0.0 (from beautifulsoup4)\n",
            "  Downloading typing_extensions-4.13.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Using cached beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
            "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
            "Downloading typing_extensions-4.13.0-py3-none-any.whl (45 kB)\n",
            "Installing collected packages: typing-extensions, soupsieve, beautifulsoup4\n",
            "Successfully installed beautifulsoup4-4.13.3 soupsieve-2.6 typing-extensions-4.13.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install PyPDF2\n",
        "%pip install requests\n",
        "%pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2ts5t5MU9rFS",
        "outputId": "f211b6bf-0e60-43a2-9d02-6b18363d6e07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 50 PDF links.\n",
            "Downloaded: s13635-025-00195-6.pdf\n",
            "Metadata for s13635-025-00195-6.pdf saved to papers\\2025-03-27\\metadata.csv\n",
            "Downloaded: s13635-025-00191-w.pdf\n",
            "Metadata for s13635-025-00191-w.pdf saved to papers\\2025-03-27\\metadata.csv\n",
            "Downloaded: s13635-025-00197-4.pdf\n",
            "Metadata for s13635-025-00197-4.pdf saved to papers\\2025-03-27\\metadata.csv\n",
            "Downloaded: s13635-024-00185-0.pdf\n",
            "Metadata for s13635-024-00185-0.pdf saved to papers\\2025-03-27\\metadata.csv\n",
            "Downloaded: s13635-025-00194-7.pdf\n",
            "Metadata for s13635-025-00194-7.pdf saved to papers\\2025-03-27\\metadata.csv\n",
            "Reached maximum download limit of 5. Stopping.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import PyPDF2\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import urljoin\n",
        "import csv\n",
        "\n",
        "def fetch_and_extract_metadata(url, download_directory=\"papers\", max_downloads=None):\n",
        "    \"\"\"\n",
        "    Fetches PDF files from a given URL, downloads them, and extracts their metadata.\n",
        "    Handles potential errors during the process and includes a retry mechanism.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the page containing the PDF links.\n",
        "        download_directory (str, optional): The directory where PDFs will be downloaded.\n",
        "            Defaults to \"papers\".\n",
        "        max_downloads (int, optional): The maximum number of PDFs to download.\n",
        "            If None, all PDFs will be downloaded. Defaults to None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create the download directory if it doesn't exist\n",
        "        if not os.path.exists(download_directory):\n",
        "            os.makedirs(download_directory)\n",
        "\n",
        "        # Send an HTTP request to the URL\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Find all anchor tags that might contain PDF links.  This is made more\n",
        "        # robust by looking for hrefs ending in .pdf, and by using urljoin.\n",
        "        pdf_links = [urljoin(url, a['href']) for a in soup.find_all('a', href=True) if a['href'].lower().endswith('.pdf')]\n",
        "\n",
        "        if not pdf_links:\n",
        "            print(f\"No PDF links found on the page: {url}\")\n",
        "            return\n",
        "\n",
        "        print(f\"Found {len(pdf_links)} PDF links.\")\n",
        "\n",
        "        # Iterate through each PDF link\n",
        "        download_count = 0\n",
        "        for pdf_url in pdf_links:\n",
        "            if max_downloads is not None and download_count >= max_downloads:\n",
        "                print(f\"Reached maximum download limit of {max_downloads}. Stopping.\")\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                # Get the PDF content with a timeout\n",
        "                pdf_response = requests.get(pdf_url, timeout=30)\n",
        "                pdf_response.raise_for_status()\n",
        "\n",
        "                # Extract the filename from the URL, handling potential issues\n",
        "                try:\n",
        "                    filename = os.path.basename(pdf_url)\n",
        "                    if not filename:\n",
        "                        filename = \"unnamed_pdf_\" + str(time.time()) + \".pdf\"  # Generate a unique name\n",
        "                except:\n",
        "                    filename = \"unnamed_pdf_\" + str(time.time()) + \".pdf\"\n",
        "\n",
        "                filepath = os.path.join(download_directory, filename)\n",
        "\n",
        "                # Write the PDF content to a file\n",
        "                with open(filepath, 'wb') as f:\n",
        "                    f.write(pdf_response.content)\n",
        "\n",
        "                print(f\"Downloaded: {filename}\")\n",
        "                download_count += 1\n",
        "\n",
        "                # Extract metadata from the downloaded PDF and save to a CSV file\n",
        "                try:\n",
        "                    with open(filepath, 'rb') as pdf_file:\n",
        "                        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "                        metadata = pdf_reader.metadata\n",
        "                        if metadata:\n",
        "                            # Prepare metadata for CSV\n",
        "                            metadata_dict = {key: value for key, value in metadata.items()}\n",
        "                            metadata_dict['filename'] = filename\n",
        "\n",
        "                            # Define the CSV file path\n",
        "                            csv_filepath = os.path.join(download_directory, \"metadata.csv\")\n",
        "\n",
        "                            # Write metadata to the CSV file\n",
        "                            write_header = not os.path.exists(csv_filepath)  # Check if header is needed\n",
        "                            with open(csv_filepath, mode='a', newline='', encoding='utf-8') as csv_file:\n",
        "                                writer = csv.DictWriter(csv_file, fieldnames=metadata_dict.keys())\n",
        "                                if write_header:\n",
        "                                    writer.writeheader()\n",
        "                                writer.writerow(metadata_dict)\n",
        "\n",
        "                            print(f\"Metadata for {filename} saved to {csv_filepath}\")\n",
        "                        else:\n",
        "                            print(f\"No metadata found in {filename}\")\n",
        "                except PyPDF2.errors.PdfReadError:\n",
        "                    print(f\"Error reading PDF: {filename}. Skipping metadata extraction.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error extracting metadata from {filename}: {e}\")\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error downloading PDF from {pdf_url}: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"An unexpected error occurred while processing {pdf_url}: {e}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching the URL {url}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "def get_daily_download_path():\n",
        "    \"\"\"\n",
        "    Generates a daily download path based on the current date.\n",
        "\n",
        "    Returns:\n",
        "        str: The path for the daily download directory (e.g., \"papers/2024-07-24\").\n",
        "    \"\"\"\n",
        "    now = datetime.now()\n",
        "    date_str = now.strftime(\"%Y-%m-%d\")\n",
        "    return os.path.join(\"papers\", date_str)\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the script.  Demonstrates daily download and error handling.\n",
        "    \"\"\"\n",
        "    url = \"https://jis-eurasipjournals.springeropen.com/articles\"\n",
        "    daily_download_path = get_daily_download_path()\n",
        "    fetch_and_extract_metadata(url, daily_download_path, max_downloads=5)  # Limiting to 5 downloads\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
